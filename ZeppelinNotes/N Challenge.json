{"paragraphs":[{"text":"%md\n#Cover\n\nThis is my submission for the data-science puzzle. I'll walk you through the excercise step-by-step, but without much depth into the techiniques. The tools used here are pretty standard, except for being in Scala. I didn't try to do any carefull data analysis, I just placed everything in a \"default\" pipeline and checked the results. \n\nIf you would like to run this notebook yourselves, you'll need to import it in Zeppelin server. I hope that is not a show stopper. Anyway, I bet Capaverde will be glad to set up one for you :)\n\nIf there's any concerns, let me know(leandro_machado@live.com).","dateUpdated":"2016-06-27T22:49:27-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467076236915_1524122289","id":"20160627-211036_16477356","result":{"code":"SUCCESS","type":"HTML","msg":"<h1>Cover</h1>\n<p>This is my submission for the data-science puzzle. I'll walk you through the excercise step-by-step, but without much depth into the techiniques. The tools used here are pretty standard, except for being in Scala. I didn't try to do any carefull data analysis, I just placed everything in a &ldquo;default&rdquo; pipeline and checked the results.</p>\n<p>If you would like to run this notebook yourselves, you'll need to import it in Zeppelin server. I hope that is not a show stopper. Anyway, I bet Capaverde will be glad to set up one for you :)</p>\n<p>If there's any concerns, let me know(leandro_machado@live.com).</p>\n"},"dateCreated":"2016-06-27T21:10:36-0400","dateStarted":"2016-06-27T22:48:20-0400","dateFinished":"2016-06-27T22:48:20-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2787"},{"text":"%dep\nz.reset()\nz.addRepo(\"Spark Packages Repo\").url(\"http://dl.bintray.com/spark-packages/maven\")\nz.load(\"com.databricks:spark-csv_2.10:1.2.0\")","dateUpdated":"2016-06-27T15:57:44-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466764362935_-152998740","id":"20160624-063242_17155840","result":{"code":"SUCCESS","type":"TEXT","msg":"DepInterpreter(%dep) deprecated. Remove dependencies and repositories through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Add repository through GUI interpreter menu instead.\nDepInterpreter(%dep) deprecated. Load dependency through GUI interpreter menu instead.\nres0: org.apache.zeppelin.dep.Dependency = org.apache.zeppelin.dep.Dependency@1bfb005\n"},"dateCreated":"2016-06-24T06:32:42-0400","dateStarted":"2016-06-27T15:57:44-0400","dateFinished":"2016-06-27T15:57:59-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2788"},{"text":"%md\nThroughout this exercise I'll be using Spark's DataFrames and the MLlib library to train, predict and evaluate the results.\nFirst let's get the data in. To read it, I'll be using the Databricks spark-csv library.","dateUpdated":"2016-06-27T22:49:31-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466799805512_-738916138","id":"20160624-162325_6611594","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Throughout this exercise I'll be using Spark's DataFrames and the MLlib library to train, predict and evaluate the results.\n<br  />First let's get the data in. To read it I'm using the Databricks spark-csv library.</p>\n"},"dateCreated":"2016-06-24T16:23:25-0400","dateStarted":"2016-06-27T15:48:08-0400","dateFinished":"2016-06-27T15:48:09-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2789"},{"text":"val train_data = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .load(\"/home/leandro/Downloads/data-science-puzzle/train.csv\")\n    \nval test_data = sqlContext.read\n    .format(\"com.databricks.spark.csv\")\n    .option(\"header\", \"true\") // Use first line of all files as header\n    .option(\"inferSchema\", \"true\") // Automatically infer data types\n    .load(\"/home/leandro/Downloads/data-science-puzzle/test.csv\")","dateUpdated":"2016-06-27T15:57:49-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466766183244_-1463151715","id":"20160624-070303_24582667","result":{"code":"SUCCESS","type":"TEXT","msg":"train_data: org.apache.spark.sql.DataFrame = [016399044a: int, 023c68873b: string, 0342faceb5: int, 04e7268385: int, 06888ceac9: int, 072b7e8f27: double, 087235d61e: int, 0b846350ef: double, 0e2ab0831c: double, 12eda2d982: double, 136c1727c3: double, 173b6590ae: double, 174825d438: int, 1f222e3669: double, 1f3058af83: int, 1fa099bb01: int, 20f1afc5c7: double, 253eb5ef11: double, 25bbf0e7e7: int, 2719b72c0d: double, 298ed82b22: double, 29bbd86997: double, 2a457d15d9: int, 2bc6ab42f7: double, 2d7fe4693a: double, 2e874bc151: double, 361f93f4d1: string, 384bec5dd1: int, 3df2300fa2: double, 3e200bf766: int, 3eb53ae932: double, 435dec85e2: double, 4468394575: double, 49756d8e0f: double, 4fc17427c8: double, 55907cc1de: double, 55cf3f7627: double, 56371466d7: int, 5b862c0a8f: int, 5f360995ef: i...test_data: org.apache.spark.sql.DataFrame = [016399044a: int, 023c68873b: string, 0342faceb5: int, 04e7268385: int, 06888ceac9: int, 072b7e8f27: double, 087235d61e: int, 0b846350ef: double, 0e2ab0831c: double, 12eda2d982: double, 136c1727c3: double, 173b6590ae: double, 174825d438: int, 1f222e3669: double, 1f3058af83: int, 1fa099bb01: int, 20f1afc5c7: double, 253eb5ef11: double, 25bbf0e7e7: int, 2719b72c0d: double, 298ed82b22: double, 29bbd86997: double, 2a457d15d9: int, 2bc6ab42f7: double, 2d7fe4693a: double, 2e874bc151: double, 361f93f4d1: string, 384bec5dd1: int, 3df2300fa2: double, 3e200bf766: int, 3eb53ae932: double, 435dec85e2: double, 4468394575: double, 49756d8e0f: double, 4fc17427c8: double, 55907cc1de: double, 55cf3f7627: double, 56371466d7: int, 5b862c0a8f: int, 5f360995ef: in..."},"dateCreated":"2016-06-24T07:03:03-0400","dateStarted":"2016-06-27T15:57:49-0400","dateFinished":"2016-06-27T15:58:43-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2790"},{"text":"%md\nNow we have to prepare the features vector by transforming the categorical features and assembling everything into a \"features\" column. Since we will be reusing this transformations I'm gonna save them as a pipeline.\n\nAs you already know there's a bunch of features for each observation(100+). To speed up things I'll use PCA to pick the top 25 variables. \n\nNot a very scientific way o pick the top K, but we just don't want to wait all day for the output, right?","dateUpdated":"2016-06-27T22:49:34-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466799902907_-1664082356","id":"20160624-162502_899752","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now we have to prepare the features vector by transforming the categorical features and assembling everything into a &ldquo;features&rdquo; column. Since we will be reusing this transformations I'm gonna save them as a pipeline.</p>\n<p>As you already know there's a bunch of features for each observation(100+). To speed up things I'll use PCA to pick the top 25 variables.</p>\n<p>Not a very scientific way o pick the top K, but we just don't want to wait all day for the output, right?</p>\n"},"dateCreated":"2016-06-24T16:25:02-0400","dateStarted":"2016-06-27T22:40:24-0400","dateFinished":"2016-06-27T22:40:24-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2791"},{"text":"import org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\nimport org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\nimport org.apache.spark.ml.feature.PCA\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\n\n//Transforming the categorical features\nval string_cols = test_data.dtypes.collect{ \n    case (col,_type) if _type == \"StringType\" => col\n}\n\nval stringIndexers = string_cols.map{\n    col => new StringIndexer().setInputCol(col).setOutputCol(col+\"_t\")\n}\n\n//Packing the features column\nval featuresCols = train_data.columns.filterNot(Set(\"id\", \"target\").contains(_)).filterNot(string_cols.contains(_)) ++ stringIndexers.map(_.getOutputCol)\n\nval vectorAssembler = new VectorAssembler().setInputCols(featuresCols).setOutputCol(\"features\")\n\n//Top 25 features\nval pca = new PCA().setInputCol(\"features\").setOutputCol(\"pcaFeatures\").setK(25)\n\n// Setting a pipeline for the Dataframe\nval stages: Array[PipelineStage] = stringIndexers.toArray[PipelineStage] :+ vectorAssembler :+ pca\n\nval pipeline = new Pipeline().setStages(stages)","dateUpdated":"2016-06-27T22:40:23-0400","config":{"colWidth":12,"graph":{"mode":"table","height":211,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466765228395_-1798276229","id":"20160624-064708_3097716","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.ml.feature.StringIndexer\nimport org.apache.spark.ml.{Pipeline, PipelineModel, PipelineStage}\nimport org.apache.spark.ml.feature.VectorAssembler\nimport org.apache.spark.ml.regression.{RandomForestRegressionModel, RandomForestRegressor}\nimport org.apache.spark.ml.regression.{GBTRegressionModel, GBTRegressor}\nimport org.apache.spark.ml.feature.PCA\nimport org.apache.spark.ml.evaluation.RegressionEvaluator\nstring_cols: Array[String] = Array(023c68873b, 361f93f4d1, 8d0606b150, 91145d159d, b835dfe10f, e16e640635, f1f0984934)\nstringIndexers: Array[org.apache.spark.ml.feature.StringIndexer] = Array(strIdx_1095e68dbe10, strIdx_20b68a238f41, strIdx_07ab1d4be5a0, strIdx_387350bd5418, strIdx_7ca2ec8cba2a, strIdx_58c6af18a30c, strIdx_cd34cd12abb8)\nfeaturesCols: Array[String] = Array(016399044a, 0342faceb5, 04e7268385, 06888ceac9, 072b7e8f27, 087235d61e, 0b846350ef, 0e2ab0831c, 12eda2d982, 136c1727c3, 173b6590ae, 174825d438, 1f222e3669, 1f3058af83, 1fa099bb01, 20f1afc5c7, 253eb5ef11, 25bbf0e7e7, 2719b72c0d, 298ed82b22, 29bbd86997, 2a457d15d9, 2bc6ab42f7, 2d7fe4693a, 2e874bc151, 384bec5dd1, 3df2300fa2, 3e200bf766, 3eb53ae932, 435dec85e2, 4468394575, 49756d8e0f, 4fc17427c8, 55907cc1de, 55cf3f7627, 56371466d7, 5b862c0a8f, 5f360995ef, 60ec1426ce, 63bcf89b1d, 6516422788, 65aed7dc1f, 6db53d265a, 7734c0c22f, 7743f273c2, 779d13189e, 77b3b41efa, 7841b6a5b1, 789b5244a9, 7925993f42, 7cb7913148, 7fe6cb4c98, 8311343404, 87b982928b, 8a21502326, 8c2e088a3d, 8de0382f02, 8f5f7c556a, 96c30c7eef, 96e6f0be58, 98475257f7, 99d44111c9, 9a575e82a4, 9b6e0...vectorAssembler: org.apache.spark.ml.feature.VectorAssembler = vecAssembler_f7f71957d534\npca: org.apache.spark.ml.feature.PCA = pca_d4b78549d9f5\nstages: Array[org.apache.spark.ml.PipelineStage] = Array(strIdx_1095e68dbe10, strIdx_20b68a238f41, strIdx_07ab1d4be5a0, strIdx_387350bd5418, strIdx_7ca2ec8cba2a, strIdx_58c6af18a30c, strIdx_cd34cd12abb8, vecAssembler_f7f71957d534, pca_d4b78549d9f5)\npipeline: org.apache.spark.ml.Pipeline = pipeline_955050b8510d\n"},"dateCreated":"2016-06-24T06:47:08-0400","dateStarted":"2016-06-27T21:43:10-0400","dateFinished":"2016-06-27T21:43:14-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2792"},{"text":"%md\nWith everything ready, we transform the trainingData and testData. We'll also set aside a subTraining set and a validation set from the trainingData, so can evaluate accuracy and do some hyperparameter tuning. Last, we create the evaluation object.","dateUpdated":"2016-06-27T22:49:39-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466800423161_1173791593","id":"20160624-163343_6257749","result":{"code":"SUCCESS","type":"HTML","msg":"<p>With everything ready, we transform the trainingData and testData. We'll also set aside a subTraining set and a validation set from the trainingData, so can evaluate accuracy and do some hyperparameter tuning. Last, we create the evaluation object.</p>\n"},"dateCreated":"2016-06-24T16:33:43-0400","dateStarted":"2016-06-27T14:28:28-0400","dateFinished":"2016-06-27T14:28:28-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2793"},{"text":"val trainingData = pipeline.fit(train_data).transform(train_data).select(\"id\", \"pcaFeatures\", \"target\").cache\nval testData = pipeline.fit(test_data).transform(test_data).select(\"id\", \"pcaFeatures\").cache\nval evaluator = new RegressionEvaluator().setLabelCol(\"target\").setPredictionCol(\"prediction\").setMetricName(\"r2\")\n\n//Setting up two models for regression.\nval rf = new RandomForestRegressor().setMaxBins(2000).setLabelCol(\"target\").setFeaturesCol(\"pcaFeatures\")\nval gbt = new GBTRegressor().setMaxBins(2000).setLabelCol(\"target\").setFeaturesCol(\"pcaFeatures\")","dateUpdated":"2016-06-27T22:02:44-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466790423364_-1293020940","id":"20160624-134703_6875522","result":{"code":"SUCCESS","type":"TEXT","msg":"trainingData: org.apache.spark.sql.DataFrame = [id: int, pcaFeatures: vector, target: double]\ntestData: org.apache.spark.sql.DataFrame = [id: int, pcaFeatures: vector]\nevaluator: org.apache.spark.ml.evaluation.RegressionEvaluator = regEval_7697d4f3abb4\nrf: org.apache.spark.ml.regression.RandomForestRegressor = rfr_643e79a6a2fe\ngbt: org.apache.spark.ml.regression.GBTRegressor = gbtr_ac812b399f5b\n"},"dateCreated":"2016-06-24T13:47:03-0400","dateStarted":"2016-06-27T22:02:44-0400","dateFinished":"2016-06-27T22:03:30-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2794"},{"text":"%md\nFinally comes the interesting part, hyperparameter tuning and model selection. However this is expensive to run this step in my machine, so I'll subjectvely pick one of the models and se the params. I'll leave the code here so you get the idea, but this is the kind of thing that needs a powerful set of machines.\n\nBasically we´ll just tune a RandomForestRegressor and a GBTRegressor, with RandomSearch, then select whichever yields the highest r2 according to the Confidence Intervals. If both look the same, I'll would pick RF because it's a more stable model with a more efficient training.\n\nThe ConfidenceIntervals would also give us a good idea of the models stability.\n\nRef: http://blog.dato.com/how-to-evaluate-machine-learning-models-part-4-hyperparameter-tuning","dateUpdated":"2016-06-27T22:49:45-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466800879259_-1965559694","id":"20160624-164119_10327258","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Finally comes the interesting part, hyperparameter tuning and model selection. However this is expensive to run this step in my machine, so I'll subjectvely pick one of the models and se the params. I'll leave the code here so you get the idea, but this is the kind of thing that needs a powerful set of machines.</p>\n<p>Basically we´ll just tune a RandomForestRegressor and a GBTRegressor, with RandomSearch, then select whichever yields the highest r2 according to the Confidence Intervals. If both look the same, I'll would pick RF because it's a more stable model with a more efficient training.</p>\n<p>The ConfidenceIntervals would also give us a good idea of the models stability.</p>\n<p>Ref: http://blog.dato.com/how-to-evaluate-machine-learning-models-part-4-hyperparameter-tuning</p>\n"},"dateCreated":"2016-06-24T16:41:19-0400","dateStarted":"2016-06-27T20:45:58-0400","dateFinished":"2016-06-27T20:45:58-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2795"},{"text":"import org.apache.spark.sql.DataFrame\n\ndef RFhyperParameterTuning(train: DataFrame, test: DataFrame, evaluator: RegressionEvaluator, rf: RandomForestRegressor) = {\n    val depths = (10 to 30)\n    val trees = (30 to 80)\n    val pairs = for{\n        d <- depths\n        t <- trees\n    } yield (d,t)\n    val results = sc.parallelize(pairs).takeSample(false, 60).map{\n        case (d, t) =>\n            val _rf = rf.setMaxDepth(d).setNumTrees(t)\n            (evaluator.evaluate(_rf.fit(train).transform(test)), (d,t))\n    }.sortBy(_._1)\n    results.reverse\n}\n\ndef GBThyperParameterTuning(train: DataFrame, test: DataFrame, evaluator: RegressionEvaluator, gbt: GBTRegressor) = {\n    val depths = (10 to 30)\n    val iterations = (10 to 50)\n    val pairs = for{\n        d <- depths\n        i <- iterations\n    } yield (d,i)\n    val results = sc.parallelize(pairs).takeSample(false, 60).map{\n        case (d, i) =>\n            val _gbt = gbt.setMaxDepth(d).setMaxIter(i)\n            (evaluator.evaluate(_gbt.fit(train).transform(test)), (d,i))\n    }.sortBy(_._1)\n    results.reverse\n}\n\nval rfHyperParams = RFhyperParameterTuning(subTraining, validation, evaluator, rf).head._2\nval gbtHyperParams = GBThyperParameterTuning(subTraining, validation, evaluator, gbt).head._2\n\nval bestRFModel = rf.setMaxDepth(rfHyperParams._1).setNumTrees(rfHyperParams._2)\nval bestGBTModel = gbt.setMaxDepth(gbtHyperParams._1).setMaxIter(gbtHyperParams._2)\n\n//Just visually compare both and decide. If there were more models and too much CI intersection, then we would have to do an hypothesis test.\nval RFpredictionCI = (1 to 100).map{ i=>\n    val bootstrapSample = trainingData.sample(true, 1.0)\n    val Array(subTraining, validation) = trainingData.randomSplit(Array(0.7,0.3))\n    evaluator.evaluate(bestRFModel.fit(subTraining).transform(validation))\n}\n\nval GBTpredictionCI = (1 to 100).map{ i=>\n    val bootstrapSample = trainingData.sample(true, 1.0)\n    val Array(subTraining, validation) = trainingData.randomSplit(Array(0.7,0.3))\n    evaluator.evaluate(bestGBTModel.fit(subTraining).transform(validation))\n}\n","dateUpdated":"2016-06-27T20:44:35-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466787508908_-45235332","id":"20160624-125828_8105930","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.sql.DataFrame\nRFhyperParameterTuning: (train: org.apache.spark.sql.DataFrame, test: org.apache.spark.sql.DataFrame, evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator, rf: org.apache.spark.ml.regression.RandomForestRegressor)Array[(Double, (Int, Int))]\nGBThyperParameterTuning: (train: org.apache.spark.sql.DataFrame, test: org.apache.spark.sql.DataFrame, evaluator: org.apache.spark.ml.evaluation.RegressionEvaluator, gbt: org.apache.spark.ml.regression.GBTRegressor)Array[(Double, (Int, Int))]\n"},"dateCreated":"2016-06-24T12:58:28-0400","dateStarted":"2016-06-24T14:17:11-0400","dateFinished":"2016-06-24T14:17:23-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2796"},{"text":"%md\nForgeting all the idealism above, we'll just pick a predictor and run a few evaluations on the trainingData to get the expected R2. Again, I'll pick RandomForest because it's more difficult to pick bad params for them (+trees -> -variance). GBT has a lot more of potential, but I won't risk choosing it without a proper Hyperparamenter tuning.\n\nHaving chosen RandomForest as the model, with a maxDepth of 10 and 30 trees, we split the trainingData 70%/30% randomly to evaluate precision. To get a better idea of the variance, we'll do that 10 times, re-randomizing at each step.\n\nThen we just output the predictions.\n","dateUpdated":"2016-06-27T22:49:57-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466802456201_1745423358","id":"20160624-170736_18510756","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Forgeting all the idealism above, we'll just pick a predictor and run a few evaluations on trainingData to get the expected R2. Again, I'll pick RandomForest because it's more difficult to pick bad params for them (+trees -> -variance). GBT has a lot more of potential, but I won't risk choosing it without a proper Hyperparamenter tuning.</p>\n<p>Having chosen RandomForest as the model, with a maxDepth of 10 and 30 trees, we split the trainingData 70%/30% randomly to evaluate precision. To get a better idea of the variance, we'll do that 10 times.</p>\n<p>Then we output the predictions.</p>\n"},"dateCreated":"2016-06-24T17:07:36-0400","dateStarted":"2016-06-27T22:30:11-0400","dateFinished":"2016-06-27T22:30:11-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2797"},{"text":"val predictor = rf.setMaxDepth(11).setNumTrees(31) //Primetime!\nval evaluation = (1 to 10).map{ i =>\n    //Re-randomizing to get a sense of the variance\n    val Array(subTraining, validation) = trainingData.randomSplit(Array(0.7, 0.3))\n    evaluator.evaluate(predictor.fit(subTraining).transform(validation))\n}\nval predictions = predictor.fit(trainingData).transform(testData).select(\"id\", \"prediction\")\n//Write the predictions out\npredictions.coalesce(1).write.format(\"com.databricks.spark.csv\").option(\"header\", \"true\").save(\"/home/leandro/Downloads/data-science-puzzle/prediction.csv\")","dateUpdated":"2016-06-27T22:08:12-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466773832303_-242649488","id":"20160624-091032_12181331","result":{"code":"SUCCESS","type":"TEXT","msg":"predictor: rf.type = rfr_643e79a6a2fe\nevaluation: scala.collection.immutable.IndexedSeq[Double] = Vector(0.11291924342862891, 0.13078584382477376, 0.12421125107889952, 0.12480122560042384, 0.12903303289171464, 0.1003477152216693, 0.13403923757429548, 0.1338260935037161, 0.13606796813429778, 0.1390889790900378)\npredictions: org.apache.spark.sql.DataFrame = [id: int, prediction: double]\n"},"dateCreated":"2016-06-24T09:10:32-0400","dateStarted":"2016-06-27T22:08:13-0400","dateFinished":"2016-06-27T22:53:08-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2798"},{"text":"%md\nAs you can see, the model is not very accurate. It will problably predict with a **R2** around **0.12 +- 0.01**. It's kind of difficult to know if this is actually difficult to predict, like very sparse and random, or if I messed up something in the process. I don't think the hyperparameter tuning would improve that much and it is difficult to iterate over the problem without a semantic understanding of the features. \n\nNonetheless, above is the result and I hope you have enjoyed the walkthrough.","dateUpdated":"2016-06-27T22:50:03-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467076687762_688287335","id":"20160627-211807_13386474","result":{"code":"SUCCESS","type":"HTML","msg":"<p>As you can see, the model is not very accurate. It will problably predict with a <strong>R2</strong> around <strong>0.12 +- 0.01</strong>. It's kind of difficult to know if this is actually difficult to predict, like very sparse and random, or if I messed up something in the process. I don't think the hyperparameter tuning would improve that much and it is difficult to iterate over the problem without a semantic understanding of the features.</p>\n<p>Nonetheless, above is the result and I hope you have enjoyed the walkthrough.</p>\n"},"dateCreated":"2016-06-27T21:18:07-0400","dateStarted":"2016-06-27T22:39:31-0400","dateFinished":"2016-06-27T22:39:31-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:2799"},{"config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1467082786952_482180631","id":"20160627-225946_10631686","dateCreated":"2016-06-27T22:59:46-0400","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:2800"}],"name":"NuBank Challenge","id":"2BRWM48EC","angularObjects":{"2BNJ5N8AT:shared_process":[],"2BPBS9EZZ:shared_process":[],"2BQDHJHDT:shared_process":[],"2BNQ93BVB:shared_process":[],"2BNTHW7XU:shared_process":[],"2BRCW47J5:shared_process":[],"2BR8XWEBQ:shared_process":[],"2BPKU9GYQ:shared_process":[],"2BN15VD5G:shared_process":[],"2BN2MBF7N:shared_process":[],"2BP526E62:shared_process":[],"2BQEYENWT:shared_process":[],"2BNNB12EV:shared_process":[],"2BNY11BJD:shared_process":[],"2BMJVRXQ2:shared_process":[],"2BQ7V2RPJ:shared_process":[],"2BQAS8ZMG:shared_process":[],"2BPMB5JN6:shared_process":[]},"config":{"looknfeel":"simple"},"info":{}}