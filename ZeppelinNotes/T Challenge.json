{"paragraphs":[{"text":"%md\n-\nIn this notebook Iĺl be descring how approached the challenge. I decided to develop the solution in Zeppelin Notebook using the Scala PL and Spark+MLlib for modeling. the first step is to read the data:","dateUpdated":"2016-06-18T02:27:00-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466223154831_-833444036","id":"20160618-001234_29200238","result":{"code":"SUCCESS","type":"HTML","msg":"<p>-\n<br  />In this notebook Iĺl be descring how approached the challenge. I decided to develop the solution in Zeppelin Notebook using the Scala PL and Spark+MLlib for modeling. the first step is to read the data:</p>\n"},"dateCreated":"2016-06-18T00:12:34-0400","dateStarted":"2016-06-18T01:24:10-0400","dateFinished":"2016-06-18T01:24:10-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1331"},{"text":"val training_data = sc.textFile(\"/home/leandro/Downloads/training_progress_predictor-3.csv\").cache()\nval test_data = sc.textFile(\"/home/leandro/Downloads/test_progress_predictor-4.csv\").cache()\n\nval header = training_data.first\nval cleanTraining = training_data.filter(_ != header)\nval headerTest = test_data.first\nval cleanTest = test_data.filter(_ != headerTest)\nval headerList = header.split(\",\")","dateUpdated":"2016-06-18T02:26:56-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466118545890_-423364435","id":"20160616-190905_19211658","result":{"code":"SUCCESS","type":"TEXT","msg":"training_data: org.apache.spark.rdd.RDD[String] = /home/leandro/Downloads/training_progress_predictor-3.csv MapPartitionsRDD[2668] at textFile at <console>:106\ntest_data: org.apache.spark.rdd.RDD[String] = /home/leandro/Downloads/test_progress_predictor-4.csv MapPartitionsRDD[2670] at textFile at <console>:106\nheader: String = user,revenue,units,ls_date,tsls,rating,ttp,total_sessions,completed,completed_post,win_rate,tries,device,tbs,tsad\ncleanTraining: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2671] at filter at <console>:110\nheaderTest: String = user,revenue,units,ls_date,tsls,rating,ttp,total_sessions,completed,win_rate,tries,device,tbs,tsad\ncleanTest: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2672] at filter at <console>:110\nheaderList: Array[String] = Array(user, revenue, units, ls_date, tsls, rating, ttp, total_sessions, completed, completed_post, win_rate, tries, device, tbs, tsad)\n"},"dateCreated":"2016-06-16T19:09:05-0400","dateStarted":"2016-06-18T01:10:22-0400","dateFinished":"2016-06-18T01:10:30-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1332"},{"text":"%md\nI'm transforming each line in a object to facilitate its manipulation later. The fields that can have NULL values will be transformed into optional fields and be dealt with later","dateUpdated":"2016-06-18T02:27:06-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466223219295_-1062076827","id":"20160618-001339_9153277","result":{"code":"SUCCESS","type":"HTML","msg":"<p>I'm transforming each line in a object to facilitate its manipulation later. The fields that can have NULL values will be transformed into optional fields and be dealt with later</p>\n"},"dateCreated":"2016-06-18T00:13:39-0400","dateStarted":"2016-06-18T00:33:34-0400","dateFinished":"2016-06-18T00:33:34-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1333"},{"text":"//Theres some NULL on the middle\n//Counting the NULL to know the option\nval countNull = cleanTraining.flatMap(line =>line.split(\",\").zipWithIndex.filter(_._1 == \"NULL\").map(zip => (zip._2, zip._1))).groupByKey.mapValues(_.size).collectAsMap\ncountNull.map{\n    case (index, nulls) => headerList(index) -> nulls\n}.map(println(_))\n\nimport scala.util.Try\n\ncase class UserInfo(id: Int, revenue: Option[Double], units: Option[Double], timeSinceLastSession: Option[Int], rating: Int, totalTimePlayed: Int, totalSessions: Int, \n                    completedPercentage: Option[Double], completedPercentageAfterTwoDays: Option[Double], winRate: Option[Double], tries: Option[Double], device: String, \n                    timeBetweenSessions: Option[Double], timeSinceActivation: Int)\n\ndef createUserInfo(row: Seq[String]): UserInfo = {\n        //Treating nulls\n        val revenue = Try(row(1).toDouble).toOption \n        val units = Try(row(2).toDouble).toOption\n        val tsls = Try(row(4).toInt).toOption\n        val cp = Try(row(8).toDouble).toOption\n        val cpl = Try(row(9).toDouble).toOption\n        val wr = Try(row(10).toDouble).toOption\n        val tries = Try(row(11).toDouble).toOption\n        val tbs = Try(row(13).toDouble).toOption\n        UserInfo(row(0).toInt, revenue, units, tsls, row(5).toInt, row(6).toInt, row(7).toInt, cp, cpl, wr, tries, row(12), tbs, row(14).toInt)\n    }\n\ndef createUserInfoForTest(row: Seq[String]): UserInfo = {\n        //Treating nulls\n        val revenue = Try(row(1).toDouble).toOption \n        val units = Try(row(2).toDouble).toOption\n        val tsls = Try(row(4).toInt).toOption\n        val cp = Try(row(8).toDouble).toOption\n        val cpl = None\n        val wr = Try(row(9).toDouble).toOption\n        val tries = Try(row(10).toDouble).toOption\n        val tbs = Try(row(12).toDouble).toOption\n        UserInfo(row(0).toInt, revenue, units, tsls, row(5).toInt, row(6).toInt, row(7).toInt, cp, cpl, wr, tries, row(11), tbs, row(13).toInt)\n    }\n    \nval trainingUserInfos = cleanTraining.map(line => createUserInfo(line.split(\",\"))).cache\nval testUserInfos = cleanTest.map(line => createUserInfoForTest(line.split(\",\"))).cache\n\ntrainingUserInfos.first\ntestUserInfos.first","dateUpdated":"2016-06-18T02:26:56-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466118659835_1009610680","id":"20160616-191059_23123521","result":{"code":"SUCCESS","type":"TEXT","msg":"countNull: scala.collection.Map[Int,Int] = Map(8 -> 191, 2 -> 6970, 11 -> 739, 4 -> 2233, 13 -> 2806, 10 -> 739, 1 -> 6970, 9 -> 175, 3 -> 2233)\n(win_rate,739)\n(ls_date,2233)\n(completed,191)\n(revenue,6970)\n(units,6970)\n(tries,739)\n(completed_post,175)\n(tbs,2806)\n(tsls,2233)\nres36: Iterable[Unit] = List((), (), (), (), (), (), (), (), ())\nimport scala.util.Try\ndefined class UserInfo\ncreateUserInfo: (row: Seq[String])UserInfo\ncreateUserInfoForTest: (row: Seq[String])UserInfo\ntrainingUserInfos: org.apache.spark.rdd.RDD[UserInfo] = MapPartitionsRDD[2676] at map at <console>:118\ntestUserInfos: org.apache.spark.rdd.RDD[UserInfo] = MapPartitionsRDD[2677] at map at <console>:118\nres37: UserInfo = UserInfo(3567,None,None,Some(90252),-1,548,3,Some(0.155555555556),Some(0.155555555556),Some(0.25),Some(4.0),ipod,Some(46193.0),182922)\nres38: UserInfo = UserInfo(1,None,None,Some(17408),0,30476,8,Some(0.711111111111),None,Some(0.272727272727),Some(88.0),ipad,Some(18076.7142857),126965)\n"},"dateCreated":"2016-06-16T19:10:59-0400","dateStarted":"2016-06-18T01:10:43-0400","dateFinished":"2016-06-18T01:10:56-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1334"},{"text":"%md\nOur only categorical field is the device attribute, so I'm checking how many categories we have. It's also important to verify if we have duplicate users.","dateUpdated":"2016-06-18T02:27:12-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466223698628_426421433","id":"20160618-002138_21894370","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Our only categorical field is the device attribute, so I'm checking how many categories we have. It's also important to verify if we have duplicate users.</p>\n"},"dateCreated":"2016-06-18T00:21:38-0400","dateStarted":"2016-06-18T00:33:21-0400","dateFinished":"2016-06-18T00:33:21-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1335"},{"text":"trainingUserInfos.groupBy(_.device).mapValues(_.size).collectAsMap\n//Lets see if we have more tha one info for each user(probably yes)\ntrainingUserInfos.groupBy(_.id).filter(_._2.size > 1).count\n//Actually we don't","dateUpdated":"2016-06-18T02:26:56-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466206704350_1527598353","id":"20160617-193824_24084282","result":{"code":"SUCCESS","type":"TEXT","msg":"res53: scala.collection.Map[String,Int] = Map(iphone -> 4313, ipod -> 512, ipad -> 2175)\nres54: Long = 0\n"},"dateCreated":"2016-06-17T19:38:24-0400","dateStarted":"2016-06-18T02:26:22-0400","dateFinished":"2016-06-18T02:26:35-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1336"},{"text":"%md\nNow we transform the UserInfo into a feature set. Has mentioned before, some fields here have NULL values. So we have to map this NULL to the continuous realm in a way that it makes sense for the attribute definitions.\n1. Revenue and Units if NULL can be considered 0\n2. TSLS and TBS if NULL can be considered as TSAD\n3. CompletedPercetage, WinRate and Tries if NULL can be considered 0","dateUpdated":"2016-06-18T02:27:19-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466223848715_-1307287236","id":"20160618-002408_15695997","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now we transform the UserInfo into a feature set. Has mentioned before, some fields here have NULL values. So we have to map this NULL to the continuous realm in a way that it makes sense for the attribute definitions.</p>\n<ol>\n<li>Revenue and Units if NULL can be considered 0</li>\n<li>TSLS and TBS if NULL can be considered as TSAD</li>\n<li>CompletedPercetage, WinRate and Tries if NULL can be considered 0</li>\n</ol>\n"},"dateCreated":"2016-06-18T00:24:08-0400","dateStarted":"2016-06-18T00:33:04-0400","dateFinished":"2016-06-18T00:33:04-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1337"},{"text":"import org.apache.spark.mllib.linalg.{Vectors, Vector}\nimport org.apache.spark.mllib.regression.LabeledPoint\n//We'll use scala mllib\nval deviceMap = Map(\"ipod\" -> 0, \"ipad\" -> 1, \"iphone\" -> 2)\n\ndef featureVector(userInfo: UserInfo): Vector = {\n    // If no money, just consider it to be zero\n    Vectors.dense(userInfo.revenue.getOrElse(0), userInfo.units.getOrElse(0),\n                  //If no tsls, consideser tsad\n                  userInfo.timeSinceLastSession.getOrElse[Int](userInfo.timeSinceActivation),\n                  //Moving rating up to avoid negatives\n                  userInfo.rating+1, userInfo.totalTimePlayed, userInfo.totalSessions,\n                  //If completed percetage is empty, consider it to be zero\n                  userInfo.completedPercentage.getOrElse(0),\n                  //What a looser!              (Tente outra veeezz...)       mapping to numbered labels\n                  userInfo.winRate.getOrElse(0), userInfo.tries.getOrElse(0), deviceMap(userInfo.device),\n                  userInfo.timeBetweenSessions.getOrElse[Double](userInfo.timeSinceActivation), userInfo.timeSinceActivation\n                  )\n}\n\ndef toLabeledPoint(userInfo: UserInfo): LabeledPoint = {\n    LabeledPoint(userInfo.completedPercentageAfterTwoDays.get, featureVector(userInfo))\n}\n\n//Creating training LabeledPoints\nval labeledTraining = trainingUserInfos\n    //We need labeled points, so...\n    .collect{\n        case userinfo if userinfo.completedPercentageAfterTwoDays.isDefined =>\n            toLabeledPoint(userinfo)\n    }","dateUpdated":"2016-06-18T02:26:56-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466199719790_-233403867","id":"20160617-174159_30755945","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.linalg.{Vectors, Vector}\nimport org.apache.spark.mllib.regression.LabeledPoint\ndeviceMap: scala.collection.immutable.Map[String,Int] = Map(ipod -> 0, ipad -> 1, iphone -> 2)\nfeatureVector: (userInfo: UserInfo)org.apache.spark.mllib.linalg.Vector\ntoLabeledPoint: (userInfo: UserInfo)org.apache.spark.mllib.regression.LabeledPoint\nlabeledTraining: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint] = MapPartitionsRDD[2740] at collect at <console>:133\n"},"dateCreated":"2016-06-17T17:41:59-0400","dateStarted":"2016-06-18T01:27:20-0400","dateFinished":"2016-06-18T01:27:30-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1338"},{"text":"%md\nNow it's time for the modeling:\n1.  I'll be using a RandomForest model, availble in the Spark MLlib. No feature selection is necessary as we have just a few features.\n2.  To have an ideia of the model accuracy we'll do the usual test/validation split\n3.  For hyperparameter tunning I'm doing a RandomGridSearch with 20 samples. Ideally would have to be some close to 60, but I'm short in computation and the variance is considerably low. So 20 should be enough\n4.  We'll use MSE as the main accuracy metric, but R2 is also quite important. It can give us a better ideia of the model's \"goodness of fit\". ","dateUpdated":"2016-06-18T02:26:57-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466224841203_1738443065","id":"20160618-004041_16155071","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now it's time for the modeling:</p>\n<ol>\n<li>I'll be using a RandomForest model, availble in the Spark MLlib. No feature selection is necessary as we have just a few features.</li>\n<li>To have an ideia of the model accuracy we'll do the usual test/validation split</li>\n<li>For hyperparameter tunning I'm doing a RandomGridSearch with 20 samples. Ideally would have to be some close to 60, but I'm short in computation and the variance is considerably low. So 20 should be enough</li>\n<li>We'll use MSE as the main accuracy metric, but R2 is also quite important. It can give us a better ideia of the model's &ldquo;goodness of fit&rdquo;.</li>\n</ol>\n"},"dateCreated":"2016-06-18T00:40:41-0400","dateStarted":"2016-06-18T01:01:26-0400","dateFinished":"2016-06-18T01:01:26-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1339"},{"text":"import org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\nimport org.apache.spark.rdd.RDD\n\ndef trainRFModel(training: RDD[LabeledPoint], maxDepth: Int, numTrees: Int) = {\n    val categoricalFeaturesInfo = Map[Int, Int](9 -> 3)\n    val featureSubsetStrategy = \"auto\"\n    val impurity = \"variance\"\n    val maxBins = 1000\n    RandomForest.trainRegressor(subTrain, categoricalFeaturesInfo, numTrees, featureSubsetStrategy, impurity, maxDepth, maxBins)\n}\n\ndef evaluateRFWithValidation(labeledPoints: RDD[LabeledPoint], maxDepth: Int = 4, numTrees: Int = 3) = {\n    //Split for validation\n    val split = labeledPoints.randomSplit(Array(0.7,0.3))\n    val training = split(0)\n    val validation = split(1)\n    val model = trainRFModel(training, maxDepth, numTrees)\n    val predictionsAndTruths = validation.map(label => (model.predict(label.features), label.label))\n    val metrics = new RegressionMetrics(predictionsAndTruths)\n    (metrics.meanSquaredError, metrics.r2)\n}\n\ndef hyperparameterTuning(labeledPoints: RDD[LabeledPoint]) = {\n    val sampleSize: Int = 20\n    val maxDepthList = (1 to 20)\n    val numTreesList = (1 to 30)\n    val joinedParams = for{\n        maxDepth <- maxDepthList\n        numTrees <- numTreesList\n    } yield (maxDepth, numTrees)\n    val paramsSample = sc.parallelize(joinedParams).takeSample(false, sampleSize)\n    //TODO: All of the params should be evaluated under the same split\n    //Sorting by MSE\n    paramsSample.map{\n        case (d,t) => (evaluateRFWithValidation(labeledPoints, d, t), (d,t))\n    }.sortBy(_._1._1)\n}\n","dateUpdated":"2016-06-18T02:26:57-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466202549336_-348935283","id":"20160617-182909_21484614","result":{"code":"SUCCESS","type":"TEXT","msg":"import org.apache.spark.mllib.tree.RandomForest\nimport org.apache.spark.mllib.tree.model.RandomForestModel\nimport org.apache.spark.mllib.evaluation.RegressionMetrics\nimport org.apache.spark.rdd.RDD\ntrainRFModel: (training: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint], maxDepth: Int, numTrees: Int)org.apache.spark.mllib.tree.model.RandomForestModel\nevaluateRFWithValidation: (labeledPoints: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint], maxDepth: Int, numTrees: Int)(Double, Double)\nhyperparameterTuning: (labeledPoints: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint])Array[((Double, Double), (Int, Int))]\nevaluationConfidenceIntervals: (labeledPoints: org.apache.spark.rdd.RDD[org.apache.spark.mllib.regression.LabeledPoint], maxDepth: Int, numTrees: Int)Unit\n"},"dateCreated":"2016-06-17T18:29:09-0400","dateStarted":"2016-06-18T00:08:48-0400","dateFinished":"2016-06-18T00:08:54-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1340"},{"text":"%md\n-\nBelow we have a list of the random hyperparameters tested ordered by their MSE and R2 ((MSE, R2), (depth, nrTrees)). We'll select depth=17 and nrTrees=20 as our hyperparameters, to give better balance between MSE and R2.","dateUpdated":"2016-06-18T02:27:26-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466225842447_761001903","id":"20160618-005722_15830575","result":{"code":"SUCCESS","type":"HTML","msg":"<p>-\n<br  />Below we have a list of the random hyperparameters tested ordered by their MSE and R2 ((MSE, R2), (depth, nrTrees)). We'll select depth=17 and nrTrees=20 as our hyperparameters, to give better balance between MSE and R2.</p>\n"},"dateCreated":"2016-06-18T00:57:22-0400","dateStarted":"2016-06-18T01:52:25-0400","dateFinished":"2016-06-18T01:52:25-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1341"},{"text":"hyperparameterTuning(labeledTraining)","dateUpdated":"2016-06-18T02:26:57-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466218573377_1915679922","id":"20160617-225613_23120091","result":{"code":"SUCCESS","type":"TEXT","msg":"res24: Array[((Double, Double), (Int, Int))] = Array(((0.0030512550952016874,0.9276245009026803),(18,21)), ((0.003154413486472495,0.9337034207405167),(16,15)), ((0.0032213866498529775,0.9251419982918263),(17,23)), ((0.0034918684981141627,0.916735167841789),(16,27)), ((0.0034954833987559503,0.9194409578208458),(20,14)), ((0.003563358744525225,0.9149163194091173),(11,17)), ((0.003654919143507854,0.9140865859468982),(14,15)), ((0.003805355159996909,0.9145070944022019),(17,16)), ((0.003867818580574801,0.9119562170721888),(12,26)), ((0.004032018152714507,0.9087636639856242),(19,5)), ((0.0044039693658721605,0.8970552359091456),(16,6)), ((0.004474090497735567,0.8924934409745818),(19,4)), ((0.005279588503705544,0.8857388867638243),(9,10)), ((0.005432479936454918,0.8667959251064006),(8,17)), ((0..."},"dateCreated":"2016-06-17T22:56:13-0400","dateStarted":"2016-06-18T00:09:30-0400","dateFinished":"2016-06-18T00:31:04-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1342"},{"text":"%md\nNow it's time to generate the predictions","dateUpdated":"2016-06-18T02:26:57-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466226259330_-511193621","id":"20160618-010419_27657957","result":{"code":"SUCCESS","type":"HTML","msg":"<p>Now it's time to generate the predictions</p>\n"},"dateCreated":"2016-06-18T01:04:19-0400","dateStarted":"2016-06-18T01:17:06-0400","dateFinished":"2016-06-18T01:17:06-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1343"},{"text":"val model = trainRFModel(labeledTraining, 17, 20)\n// Returning a (user, prediction) and writing to a .csv file\nval predictions = testUserInfos.map(userinfo => (userinfo.id, model.predict(featureVector(userinfo)))).map(t => t._1+\",\"+t._2)\npredictions.saveAsTextFile(\"/home/leandro/tfg_predictions.csv\")","dateUpdated":"2016-06-18T02:26:57-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/scala","editorHide":false},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466218897873_-1190374402","id":"20160617-230137_6063724","result":{"code":"SUCCESS","type":"TEXT","msg":"predictions: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[2745] at map at <console>:171\n"},"dateCreated":"2016-06-17T23:01:37-0400","dateStarted":"2016-06-18T01:32:27-0400","dateFinished":"2016-06-18T01:32:36-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1344"},{"text":"%md\nA few notes on my challenge:\n1.  I focused more on the the prediction than on exploratory analysis. There's probably some interesting things to find out about which features are strong predictors and how they correlated with the game progress.\n2.  A simple RF model worked better than I expected, both in accuracy and stability. If the results were R2 < 0.8 I would probably try out with Gradient Boosted Trees, and if the hyperparameters results had greater variance I would use Bootstrap Aggregating to stabilize the model and avoid overfitting.\n3.  I wanted to to use Bootstrap to expose a confidence interval on the prediction accuracy. However it was taking to long to run in my computer and the hyperparam results where quite \"well-behaved\" so we wouldn't have any surprises.\n4.  When you run the test data I'm expecting something around MSE = 0.0032 +- 0.0002 and R2 = 0.91 +- 0.01.\n5.  In the \"real world\" the work we wouldn't have stopped here. We would have to quantify the \"cost of error\" and improve the model until we reached the optimal cost/benefit.","dateUpdated":"2016-06-18T02:27:31-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorMode":"ace/mode/markdown","editorHide":true},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466227102555_-296480188","id":"20160618-011822_1541630","result":{"code":"SUCCESS","type":"HTML","msg":"<p>A few notes on my challenge:</p>\n<ol>\n<li>I focused more on the the prediction than on exploratory analysis. There's probably some interesting things to find out about which features are strong predictors and how they correlated with the game progress.</li>\n<li>A simple RF model worked better than I expected, both in accuracy and stability. If the results were R2 &lt; 0.8 I would probably try out with Gradient Boosted Trees, and if the hyperparameters results had greater variance I would use Bootstrap Aggregating to stabilize the model and avoid overfitting.</li>\n<li>I wanted to to use Bootstrap to expose a confidence interval on the prediction accuracy. However it was taking to long to run in my computer and the hyperparam results where quite &ldquo;well-behaved&rdquo; so we wouldn't have any surprises.</li>\n<li>When you run the test data I'm expecting something around MSE = 0.0032 +- 0.0002 and R2 = 0.91 +- 0.01.</li>\n<li>In the &ldquo;real world&rdquo; the work we wouldn't have stopped here. We would have to quantify the &ldquo;cost of error&rdquo; and improve the model until we reached the optimal cost/benefit.</li>\n</ol>\n"},"dateCreated":"2016-06-18T01:18:22-0400","dateStarted":"2016-06-18T01:59:12-0400","dateFinished":"2016-06-18T01:59:12-0400","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:1345"},{"dateUpdated":"2016-06-18T02:26:58-0400","config":{"colWidth":12,"graph":{"mode":"table","height":300,"optionOpen":false,"keys":[],"values":[],"groups":[],"scatter":{}},"enabled":true,"editorHide":false,"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"jobName":"paragraph_1466229091230_450229524","id":"20160618-015131_12011403","dateCreated":"2016-06-18T01:51:31-0400","status":"READY","progressUpdateIntervalMs":500,"$$hashKey":"object:1346"}],"name":"TFG Challenge","id":"2BQY5UE8G","angularObjects":{"2BNJ5N8AT:shared_process":[],"2BPBS9EZZ:shared_process":[],"2BQDHJHDT:shared_process":[],"2BNQ93BVB:shared_process":[],"2BNTHW7XU:shared_process":[],"2BRCW47J5:shared_process":[],"2BR8XWEBQ:shared_process":[],"2BPKU9GYQ:shared_process":[],"2BN15VD5G:shared_process":[],"2BN2MBF7N:shared_process":[],"2BP526E62:shared_process":[],"2BQEYENWT:shared_process":[],"2BNNB12EV:shared_process":[],"2BNY11BJD:shared_process":[],"2BMJVRXQ2:shared_process":[],"2BQ7V2RPJ:shared_process":[],"2BQAS8ZMG:shared_process":[],"2BPMB5JN6:shared_process":[]},"config":{"looknfeel":"simple"},"info":{}}